{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L3X7dpCKRsj"
      },
      "source": [
        "#Package#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW0kqsqbKRHt",
        "outputId": "ea443d27-8d51-4b67-ef53-6fafb86ab319"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32md:\\Bangkit\\CAPSTONE\\dataset\\Untitled0.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Bangkit/CAPSTONE/dataset/Untitled0.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Bangkit/CAPSTONE/dataset/Untitled0.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Bangkit/CAPSTONE/dataset/Untitled0.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# import pandas as pd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Bangkit/CAPSTONE/dataset/Untitled0.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "# import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers.legacy import SGD\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ZQ2auIK3-q"
      },
      "source": [
        "#load data#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBhekehsK8H4",
        "outputId": "a4a78fb1-6493-4f60-981e-3c03f549f3c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(['Hi'], 'greeting'), (['How', 'are', 'you', '?'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['What', \"'s\", 'up'], 'greeting'), (['how', 'are', 'ya'], 'greeting'), (['heyy'], 'greeting'), (['whatsup'], 'greeting'), (['?', '?', '?', '?', '?', '?', '?', '?'], 'greeting'), (['cya'], 'goodbye'), (['see', 'you'], 'goodbye'), (['bye', 'bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['I', 'am', 'Leaving'], 'goodbye'), (['Bye'], 'goodbye'), (['Have', 'a', 'Good', 'day'], 'goodbye'), (['talk', 'to', 'you', 'later'], 'goodbye'), (['ttyl'], 'goodbye'), (['i', 'got', 'to', 'go'], 'goodbye'), (['gtg'], 'goodbye'), (['what', 'is', 'the', 'name', 'of', 'your', 'developers'], 'creator'), (['what', 'is', 'the', 'name', 'of', 'your', 'creators'], 'creator'), (['what', 'is', 'the', 'name', 'of', 'the', 'developers'], 'creator'), (['what', 'is', 'the', 'name', 'of', 'the', 'creators'], 'creator'), (['who', 'created', 'you'], 'creator'), (['your', 'developers'], 'creator'), (['your', 'creators'], 'creator'), (['who', 'are', 'your', 'developers'], 'creator'), (['developers'], 'creator'), (['you', 'are', 'made', 'by'], 'creator'), (['you', 'are', 'made', 'by', 'whom'], 'creator'), (['who', 'created', 'you'], 'creator'), (['who', 'create', 'you'], 'creator'), (['creators'], 'creator'), (['who', 'made', 'you'], 'creator'), (['who', 'designed', 'you'], 'creator'), (['name'], 'name'), (['your', 'name'], 'name'), (['do', 'you', 'have', 'a', 'name'], 'name'), (['what', 'are', 'you', 'called'], 'name'), (['what', 'is', 'your', 'name'], 'name'), (['what', 'should', 'I', 'call', 'you'], 'name'), (['whats', 'your', 'name', '?'], 'name'), (['what', 'are', 'you'], 'name'), (['who', 'are', 'you'], 'name'), (['who', 'is', 'this'], 'name'), (['what', 'am', 'i', 'chatting', 'to'], 'name'), (['who', 'am', 'i', 'taking', 'to'], 'name'), (['what', 'are', 'you'], 'name'), (['timing', 'of', 'college'], 'hours'), (['what', 'is', 'college', 'timing'], 'hours'), (['working', 'days'], 'hours'), (['when', 'are', 'you', 'guys', 'open'], 'hours'), (['what', 'are', 'your', 'hours'], 'hours'), (['hours', 'of', 'operation'], 'hours'), (['when', 'is', 'the', 'college', 'open'], 'hours'), (['college', 'timing'], 'hours'), (['what', 'about', 'college', 'timing'], 'hours'), (['is', 'college', 'open', 'on', 'saturday'], 'hours'), (['tell', 'something', 'about', 'college', 'timing'], 'hours'), (['what', 'is', 'the', 'college', 'hours'], 'hours'), (['when', 'should', 'i', 'come', 'to', 'college'], 'hours'), (['when', 'should', 'i', 'attend', 'college'], 'hours'), (['what', 'is', 'my', 'college', 'time'], 'hours'), (['college', 'timing'], 'hours'), (['timing', 'college'], 'hours'), (['more', 'info'], 'number'), (['contact', 'info'], 'number'), (['how', 'to', 'contact', 'college'], 'number'), (['college', 'telephone', 'number'], 'number'), (['college', 'number'], 'number'), (['What', 'is', 'your', 'contact', 'no'], 'number'), (['Contact', 'number', '?'], 'number'), (['how', 'to', 'call', 'you'], 'number'), (['College', 'phone', 'no', '?'], 'number'), (['how', 'can', 'i', 'contact', 'you'], 'number'), (['Can', 'i', 'get', 'your', 'phone', 'number'], 'number'), (['how', 'can', 'i', 'call', 'you'], 'number'), (['phone', 'number'], 'number'), (['phone', 'no'], 'number'), (['call'], 'number'), (['list', 'of', 'courses'], 'course'), (['list', 'of', 'courses', 'offered'], 'course'), (['list', 'of', 'courses', 'offered', 'in'], 'course'), (['what', 'are', 'the', 'courses', 'offered', 'in', 'your', 'college', '?'], 'course'), (['courses', '?'], 'course'), (['courses', 'offered'], 'course'), (['courses', 'offered', 'in', '(', 'your', 'univrsity', '(', 'UNI', ')', 'name', ')'], 'course'), (['courses', 'you', 'offer'], 'course'), (['branches', '?'], 'course'), (['courses', 'available', 'at', 'UNI', '?'], 'course'), (['branches', 'available', 'at', 'your', 'college', '?'], 'course'), (['what', 'are', 'the', 'courses', 'in', 'UNI', '?'], 'course'), (['what', 'are', 'branches', 'in', 'UNI', '?'], 'course'), (['what', 'are', 'courses', 'in', 'UNI', '?'], 'course'), (['branches', 'available', 'in', 'UNI', '?'], 'course'), (['can', 'you', 'tell', 'me', 'the', 'courses', 'available', 'in', 'UNI', '?'], 'course'), (['can', 'you', 'tell', 'me', 'the', 'branches', 'available', 'in', 'UNI', '?'], 'course'), (['computer', 'engineering', '?'], 'course'), (['computer'], 'course'), (['Computer', 'engineering', '?'], 'course'), (['it'], 'course'), (['IT'], 'course'), (['Information', 'Technology'], 'course'), (['AI/Ml'], 'course'), (['Mechanical', 'engineering'], 'course'), (['Chemical', 'engineering'], 'course'), (['Civil', 'engineering'], 'course'), (['information', 'about', 'fee'], 'fees'), (['information', 'on', 'fee'], 'fees'), (['tell', 'me', 'the', 'fee'], 'fees'), (['college', 'fee'], 'fees'), (['fee', 'per', 'semester'], 'fees'), (['what', 'is', 'the', 'fee', 'of', 'each', 'semester'], 'fees'), (['what', 'is', 'the', 'fees', 'of', 'each', 'year'], 'fees'), (['what', 'is', 'fee'], 'fees'), (['what', 'is', 'the', 'fees'], 'fees'), (['how', 'much', 'is', 'the', 'fees'], 'fees'), (['fees', 'for', 'first', 'year'], 'fees'), (['fees'], 'fees'), (['about', 'the', 'fees'], 'fees'), (['tell', 'me', 'something', 'about', 'the', 'fees'], 'fees'), (['What', 'is', 'the', 'fees', 'of', 'hostel'], 'fees'), (['how', 'much', 'is', 'the', 'fees'], 'fees'), (['hostel', 'fees'], 'fees'), (['fees', 'for', 'AC', 'room'], 'fees'), (['fees', 'for', 'non-AC', 'room'], 'fees'), (['fees', 'for', 'Ac', 'room', 'for', 'girls'], 'fees'), (['fees', 'for', 'non-Ac', 'room', 'for', 'girls'], 'fees'), (['fees', 'for', 'Ac', 'room', 'for', 'boys'], 'fees'), (['fees', 'for', 'non-Ac', 'room', 'for', 'boys'], 'fees'), (['where', 'is', 'the', 'college', 'located'], 'location'), (['college', 'is', 'located', 'at'], 'location'), (['where', 'is', 'college'], 'location'), (['where', 'is', 'college', 'located'], 'location'), (['address', 'of', 'college'], 'location'), (['how', 'to', 'reach', 'college'], 'location'), (['college', 'location'], 'location'), (['college', 'address'], 'location'), (['wheres', 'the', 'college'], 'location'), (['how', 'can', 'I', 'reach', 'college'], 'location'), (['whats', 'is', 'the', 'college', 'address'], 'location'), (['what', 'is', 'the', 'address', 'of', 'college'], 'location'), (['address'], 'location'), (['location'], 'location'), (['hostel', 'facility'], 'hostel'), (['hostel', 'servive'], 'hostel'), (['hostel', 'location'], 'hostel'), (['hostel', 'address'], 'hostel'), (['hostel', 'facilities'], 'hostel'), (['hostel', 'fees'], 'hostel'), (['Does', 'college', 'provide', 'hostel'], 'hostel'), (['Is', 'there', 'any', 'hostel'], 'hostel'), (['Where', 'is', 'hostel'], 'hostel'), (['do', 'you', 'have', 'hostel'], 'hostel'), (['do', 'you', 'guys', 'have', 'hostel'], 'hostel'), (['hostel'], 'hostel'), (['hostel', 'capacity'], 'hostel'), (['what', 'is', 'the', 'hostel', 'fee'], 'hostel'), (['how', 'to', 'get', 'in', 'hostel'], 'hostel'), (['what', 'is', 'the', 'hostel', 'address'], 'hostel'), (['how', 'far', 'is', 'hostel', 'from', 'college'], 'hostel'), (['hostel', 'college', 'distance'], 'hostel'), (['where', 'is', 'the', 'hostel'], 'hostel'), (['how', 'big', 'is', 'the', 'hostel'], 'hostel'), (['distance', 'between', 'college', 'and', 'hostel'], 'hostel'), (['distance', 'between', 'hostel', 'and', 'college'], 'hostel'), (['events', 'organised'], 'event'), (['list', 'of', 'events'], 'event'), (['list', 'of', 'events', 'organised', 'in', 'college'], 'event'), (['list', 'of', 'events', 'conducted', 'in', 'college'], 'event'), (['What', 'events', 'are', 'conducted', 'in', 'college'], 'event'), (['Are', 'there', 'any', 'event', 'held', 'at', 'college'], 'event'), (['Events', '?'], 'event'), (['functions'], 'event'), (['what', 'are', 'the', 'events'], 'event'), (['tell', 'me', 'about', 'events'], 'event'), (['what', 'about', 'events'], 'event'), (['document', 'to', 'bring'], 'document'), (['documents', 'needed', 'for', 'admision'], 'document'), (['documents', 'needed', 'at', 'the', 'time', 'of', 'admission'], 'document'), (['documents', 'needed', 'during', 'admission'], 'document'), (['documents', 'required', 'for', 'admision'], 'document'), (['documents', 'required', 'at', 'the', 'time', 'of', 'admission'], 'document'), (['documents', 'required', 'during', 'admission'], 'document'), (['What', 'document', 'are', 'required', 'for', 'admission'], 'document'), (['Which', 'document', 'to', 'bring', 'for', 'admission'], 'document'), (['documents'], 'document'), (['what', 'documents', 'do', 'i', 'need'], 'document'), (['what', 'documents', 'do', 'I', 'need', 'for', 'admission'], 'document'), (['documents', 'needed'], 'document'), (['size', 'of', 'campus'], 'floors'), (['building', 'size'], 'floors'), (['How', 'many', 'floors', 'does', 'college', 'have'], 'floors'), (['floors', 'in', 'college'], 'floors'), (['floors', 'in', 'college'], 'floors'), (['how', 'tall', 'is', 'UNI', \"'s\", 'College', 'of', 'Engineering', 'college', 'building'], 'floors'), (['floors'], 'floors'), (['Syllabus', 'for', 'IT'], 'syllabus'), (['what', 'is', 'the', 'Information', 'Technology', 'syllabus'], 'syllabus'), (['syllabus'], 'syllabus'), (['timetable'], 'syllabus'), (['what', 'is', 'IT', 'syllabus'], 'syllabus'), (['syllabus'], 'syllabus'), (['What', 'is', 'next', 'lecture'], 'syllabus'), (['is', 'there', 'any', 'library'], 'library'), (['library', 'facility'], 'library'), (['library', 'facilities'], 'library'), (['do', 'you', 'have', 'library'], 'library'), (['does', 'the', 'college', 'have', 'library', 'facility'], 'library'), (['college', 'library'], 'library'), (['where', 'can', 'i', 'get', 'books'], 'library'), (['book', 'facility'], 'library'), (['Where', 'is', 'library'], 'library'), (['Library'], 'library'), (['Library', 'information'], 'library'), (['Library', 'books', 'information'], 'library'), (['Tell', 'me', 'about', 'library'], 'library'), (['how', 'many', 'libraries'], 'library'), (['how', 'is', 'college', 'infrastructure'], 'infrastructure'), (['infrastructure'], 'infrastructure'), (['college', 'infrastructure'], 'infrastructure'), (['food', 'facilities'], 'canteen'), (['canteen', 'facilities'], 'canteen'), (['canteen', 'facility'], 'canteen'), (['is', 'there', 'any', 'canteen'], 'canteen'), (['Is', 'there', 'a', 'cafetaria', 'in', 'college'], 'canteen'), (['Does', 'college', 'have', 'canteen'], 'canteen'), (['Where', 'is', 'canteen'], 'canteen'), (['where', 'is', 'cafetaria'], 'canteen'), (['canteen'], 'canteen'), (['Food'], 'canteen'), (['Cafetaria'], 'canteen'), (['food', 'menu'], 'menu'), (['food', 'in', 'canteen'], 'menu'), (['Whats', 'there', 'on', 'menu'], 'menu'), (['what', 'is', 'available', 'in', 'college', 'canteen'], 'menu'), (['what', 'foods', 'can', 'we', 'get', 'in', 'college', 'canteen'], 'menu'), (['food', 'variety'], 'menu'), (['What', 'is', 'there', 'to', 'eat', '?'], 'menu'), (['What', 'is', 'college', 'placement'], 'placement'), (['Which', 'companies', 'visit', 'in', 'college'], 'placement'), (['What', 'is', 'average', 'package'], 'placement'), (['companies', 'visit'], 'placement'), (['package'], 'placement'), (['About', 'placement'], 'placement'), (['placement'], 'placement'), (['recruitment'], 'placement'), (['companies'], 'placement'), (['Who', 'is', 'HOD'], 'ithod'), (['Where', 'is', 'HOD'], 'ithod'), (['it', 'hod'], 'ithod'), (['name', 'of', 'it', 'hod'], 'ithod'), (['Who', 'is', 'computer', 'HOD'], 'computerhod'), (['Where', 'is', 'computer', 'HOD'], 'computerhod'), (['computer', 'hod'], 'computerhod'), (['name', 'of', 'computer', 'hod'], 'computerhod'), (['Who', 'is', 'extc', 'HOD'], 'extchod'), (['Where', 'is', 'extc', 'HOD'], 'extchod'), (['extc', 'hod'], 'extchod'), (['name', 'of', 'extc', 'hod'], 'extchod'), (['what', 'is', 'the', 'name', 'of', 'principal'], 'principal'), (['whatv', 'is', 'the', 'principal', 'name'], 'principal'), (['principal', 'name'], 'principal'), (['Who', 'is', 'college', 'principal'], 'principal'), (['Where', 'is', 'principal', \"'s\", 'office'], 'principal'), (['principal'], 'principal'), (['name', 'of', 'principal'], 'principal'), (['exam', 'dates'], 'sem'), (['exam', 'schedule'], 'sem'), (['When', 'is', 'semester', 'exam'], 'sem'), (['Semester', 'exam', 'timetable'], 'sem'), (['sem'], 'sem'), (['semester'], 'sem'), (['exam'], 'sem'), (['when', 'is', 'exam'], 'sem'), (['exam', 'timetable'], 'sem'), (['exam', 'dates'], 'sem'), (['when', 'is', 'semester'], 'sem'), (['what', 'is', 'the', 'process', 'of', 'admission'], 'admission'), (['what', 'is', 'the', 'admission', 'process'], 'admission'), (['How', 'to', 'take', 'admission', 'in', 'your', 'college'], 'admission'), (['What', 'is', 'the', 'process', 'for', 'admission'], 'admission'), (['admission'], 'admission'), (['admission', 'process'], 'admission'), (['scholarship'], 'scholarship'), (['Is', 'scholarship', 'available'], 'scholarship'), (['scholarship', 'engineering'], 'scholarship'), (['scholarship', 'it'], 'scholarship'), (['scholarship', 'ce'], 'scholarship'), (['scholarship', 'mechanical'], 'scholarship'), (['scholarship', 'civil'], 'scholarship'), (['scholarship', 'chemical'], 'scholarship'), (['scholarship', 'for', 'AI/ML'], 'scholarship'), (['available', 'scholarships'], 'scholarship'), (['scholarship', 'for', 'computer', 'engineering'], 'scholarship'), (['scholarship', 'for', 'IT', 'engineering'], 'scholarship'), (['scholarship', 'for', 'mechanical', 'engineering'], 'scholarship'), (['scholarship', 'for', 'civil', 'engineering'], 'scholarship'), (['scholarship', 'for', 'chemical', 'engineering'], 'scholarship'), (['list', 'of', 'scholarship'], 'scholarship'), (['comps', 'scholarship'], 'scholarship'), (['IT', 'scholarship'], 'scholarship'), (['mechanical', 'scholarship'], 'scholarship'), (['civil', 'scholarship'], 'scholarship'), (['chemical', 'scholarship'], 'scholarship'), (['automobile', 'scholarship'], 'scholarship'), (['first', 'year', 'scholarship'], 'scholarship'), (['second', 'year', 'scholarship'], 'scholarship'), (['third', 'year', 'scholarship'], 'scholarship'), (['fourth', 'year', 'scholarship'], 'scholarship'), (['What', 'facilities', 'college', 'provide'], 'facilities'), (['College', 'facility'], 'facilities'), (['What', 'are', 'college', 'facilities'], 'facilities'), (['facilities'], 'facilities'), (['facilities', 'provided'], 'facilities'), (['max', 'number', 'of', 'students'], 'college intake'), (['number', 'of', 'seats', 'per', 'branch'], 'college intake'), (['number', 'of', 'seats', 'in', 'each', 'branch'], 'college intake'), (['maximum', 'number', 'of', 'seats'], 'college intake'), (['maximum', 'students', 'intake'], 'college intake'), (['What', 'is', 'college', 'intake'], 'college intake'), (['how', 'many', 'stundent', 'are', 'taken', 'in', 'each', 'branch'], 'college intake'), (['seat', 'allotment'], 'college intake'), (['seats'], 'college intake'), (['college', 'dress', 'code'], 'uniform'), (['college', 'dresscode'], 'uniform'), (['what', 'is', 'the', 'uniform'], 'uniform'), (['can', 'we', 'wear', 'casuals'], 'uniform'), (['Does', 'college', 'have', 'an', 'uniform'], 'uniform'), (['Is', 'there', 'any', 'uniform'], 'uniform'), (['uniform'], 'uniform'), (['what', 'about', 'uniform'], 'uniform'), (['do', 'we', 'have', 'to', 'wear', 'uniform'], 'uniform'), (['what', 'are', 'the', 'different', 'committe', 'in', 'college'], 'committee'), (['different', 'committee', 'in', 'college'], 'committee'), (['Are', 'there', 'any', 'committee', 'in', 'college'], 'committee'), (['Give', 'me', 'committee', 'details'], 'committee'), (['committee'], 'committee'), (['how', 'many', 'committee', 'are', 'there', 'in', 'college'], 'committee'), (['I', 'love', 'you'], 'random'), (['Will', 'you', 'marry', 'me'], 'random'), (['Do', 'you', 'love', 'me'], 'random'), (['fuck'], 'swear'), (['bitch'], 'swear'), (['shut', 'up'], 'swear'), (['hell'], 'swear'), (['stupid'], 'swear'), (['idiot'], 'swear'), (['dumb', 'ass'], 'swear'), (['asshole'], 'swear'), (['fucker'], 'swear'), (['holidays'], 'vacation'), (['when', 'will', 'semester', 'starts'], 'vacation'), (['when', 'will', 'semester', 'end'], 'vacation'), (['when', 'is', 'the', 'holidays'], 'vacation'), (['list', 'of', 'holidays'], 'vacation'), (['Holiday', 'in', 'these', 'year'], 'vacation'), (['holiday', 'list'], 'vacation'), (['about', 'vacations'], 'vacation'), (['about', 'holidays'], 'vacation'), (['When', 'is', 'vacation'], 'vacation'), (['When', 'is', 'holidays'], 'vacation'), (['how', 'long', 'will', 'be', 'the', 'vacation'], 'vacation'), (['sports', 'and', 'games'], 'sports'), (['give', 'sports', 'details'], 'sports'), (['sports', 'infrastructure'], 'sports'), (['sports', 'facilities'], 'sports'), (['information', 'about', 'sports'], 'sports'), (['Sports', 'activities'], 'sports'), (['please', 'provide', 'sports', 'and', 'games', 'information'], 'sports'), (['okk'], 'salutaion'), (['okie'], 'salutaion'), (['nice', 'work'], 'salutaion'), (['well', 'done'], 'salutaion'), (['good', 'job'], 'salutaion'), (['thanks', 'for', 'the', 'help'], 'salutaion'), (['Thank', 'You'], 'salutaion'), (['its', 'ok'], 'salutaion'), (['Thanks'], 'salutaion'), (['Good', 'work'], 'salutaion'), (['k'], 'salutaion'), (['ok'], 'salutaion'), (['okay'], 'salutaion'), (['what', 'can', 'you', 'do'], 'task'), (['what', 'are', 'the', 'thing', 'you', 'can', 'do'], 'task'), (['things', 'you', 'can', 'do'], 'task'), (['what', 'can', 'u', 'do', 'for', 'me'], 'task'), (['how', 'u', 'can', 'help', 'me'], 'task'), (['why', 'i', 'should', 'use', 'you'], 'task'), (['ragging'], 'ragging'), (['is', 'ragging', 'practice', 'active', 'in', 'college'], 'ragging'), (['does', 'college', 'have', 'any', 'antiragging', 'facility'], 'ragging'), (['is', 'there', 'any', 'ragging', 'cases'], 'ragging'), (['is', 'ragging', 'done', 'here'], 'ragging'), (['ragging', 'against'], 'ragging'), (['antiragging', 'facility'], 'ragging'), (['ragging', 'juniors'], 'ragging'), (['ragging', 'history'], 'ragging'), (['ragging', 'incidents'], 'ragging'), (['hod'], 'hod'), (['hod', 'name'], 'hod'), (['who', 'is', 'the', 'hod'], 'hod')]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "file_path = '/content/drive/MyDrive/intents.json'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "  data_json = json.load(file)\n",
        "\n",
        "# tag = []\n",
        "# patterns = []\n",
        "# responses = []\n",
        "# for intent in data_json['intents']:\n",
        "#     tag.extend(intent['tag'])\n",
        "#     patterns.extend(intent['patterns'])\n",
        "#     responses.extend(intent['responses'])\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = ['?', '!', '.', ',']\n",
        "\n",
        "for intent in data_json['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    word_list = nltk.word_tokenize(pattern)\n",
        "    words.extend(word_list)\n",
        "    documents.append((word_list, intent['tag']))\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "\n",
        "print(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13-wXPTZLtZw"
      },
      "source": [
        "#Preprocessing#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UinF8QZOY_sZ"
      },
      "outputs": [],
      "source": [
        "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
        "words = sorted(set(words))\n",
        "\n",
        "classes = sorted(set(classes))\n",
        "\n",
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x520NshLw0X"
      },
      "outputs": [],
      "source": [
        "# tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts(patterns)\n",
        "# tokenizer.fit_on_texts(responses)\n",
        "\n",
        "# #Proses Patterns\n",
        "# sequences_patterns = tokenizer.texts_to_sequences(patterns)\n",
        "# max_length_patterns = max(len(seq) for seq in sequences_patterns)\n",
        "# padded_sequences_patterns = pad_sequences(sequences_patterns, maxlen=max_length_patterns, padding='post', truncating='post')\n",
        "\n",
        "# #Proses Responses\n",
        "# sequences_responses = tokenizer.texts_to_sequences(responses)\n",
        "# max_length_responses = max(len(seq) for seq in sequences_responses)\n",
        "# padded_sequences_responses = pad_sequences(sequences_responses, maxlen=max_length_responses, padding='post', truncating='post')\n",
        "\n",
        "# #label Encoding kolom tags\n",
        "# label_encoder = LabelEncoder()\n",
        "# tag = label_encoder.fit_transform(tag)\n",
        "\n",
        "# # Pisahkan data\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     padded_sequences_patterns, padded_sequences_responses, test_size=0.2, random_state=42\n",
        "# )\n",
        "\n",
        "# print(\"Sequences patterns:\")\n",
        "# print(sequences_patterns)\n",
        "\n",
        "# print(\"\\nPadded Sequences patterns:\")\n",
        "# print(padded_sequences_patterns)\n",
        "\n",
        "# print(\"Sequences responses:\")\n",
        "# print(sequences_responses)\n",
        "\n",
        "# print(\"\\nPadded Sequences responses:\")\n",
        "# print(padded_sequences_responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEg8nuVqWsh2",
        "outputId": "eef40bd3-f549-458d-a2b4-9dd78b197ffa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-7785cabc9d9f>:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training = np.array(training)\n"
          ]
        }
      ],
      "source": [
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for document in documents:\n",
        "  bag = []\n",
        "  word_patterns = document[0]\n",
        "  word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "  for word in words:\n",
        "    bag.append(1) if word in word_patterns else bag.append(0)\n",
        "\n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(document[1])] = 1\n",
        "  training.append([bag, output_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "train_x = list(training[:, 0])\n",
        "train_y = list(training[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AJc5eo_c2Hh"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "sgd = SGD (learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpOdMpTEQDag"
      },
      "outputs": [],
      "source": [
        "# model = Sequential()\n",
        "# model.add(Embedding(total_words, 100, input_length=max_sequence_length))\n",
        "# model.add(LSTM(100))\n",
        "# model.add(Dense(total_words_responses, activation='softmax'))\n",
        "\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUaQe3JqQDiW",
        "outputId": "6f4f164f-0844-4fda-ee3a-2d659d61a6d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.9728\n",
            "Epoch 2/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 0.9605\n",
            "Epoch 3/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1120 - accuracy: 0.9481\n",
            "Epoch 4/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1030 - accuracy: 0.9728\n",
            "Epoch 5/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1100 - accuracy: 0.9654\n",
            "Epoch 6/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1603 - accuracy: 0.9630\n",
            "Epoch 7/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1469 - accuracy: 0.9630\n",
            "Epoch 8/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1793 - accuracy: 0.9605\n",
            "Epoch 9/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9506\n",
            "Epoch 10/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1427 - accuracy: 0.9556\n",
            "Epoch 11/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.9432\n",
            "Epoch 12/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1446 - accuracy: 0.9506\n",
            "Epoch 13/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9753\n",
            "Epoch 14/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1231 - accuracy: 0.9654\n",
            "Epoch 15/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1168 - accuracy: 0.9778\n",
            "Epoch 16/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1927 - accuracy: 0.9506\n",
            "Epoch 17/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1566 - accuracy: 0.9531\n",
            "Epoch 18/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9728\n",
            "Epoch 19/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0848 - accuracy: 0.9605\n",
            "Epoch 20/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9605\n",
            "Epoch 21/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0667 - accuracy: 0.9802\n",
            "Epoch 22/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2018 - accuracy: 0.9580\n",
            "Epoch 23/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9531\n",
            "Epoch 24/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1342 - accuracy: 0.9605\n",
            "Epoch 25/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9753\n",
            "Epoch 26/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1237 - accuracy: 0.9679\n",
            "Epoch 27/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1105 - accuracy: 0.9630\n",
            "Epoch 28/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2327 - accuracy: 0.9605\n",
            "Epoch 29/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1753 - accuracy: 0.9432\n",
            "Epoch 30/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1283 - accuracy: 0.9580\n",
            "Epoch 31/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1644 - accuracy: 0.9753\n",
            "Epoch 32/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9605\n",
            "Epoch 33/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0955 - accuracy: 0.9704\n",
            "Epoch 34/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1900 - accuracy: 0.9605\n",
            "Epoch 35/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1940 - accuracy: 0.9531\n",
            "Epoch 36/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1437 - accuracy: 0.9506\n",
            "Epoch 37/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2111 - accuracy: 0.9556\n",
            "Epoch 38/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0631 - accuracy: 0.9778\n",
            "Epoch 39/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1828 - accuracy: 0.9432\n",
            "Epoch 40/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1245 - accuracy: 0.9605\n",
            "Epoch 41/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1789 - accuracy: 0.9605\n",
            "Epoch 42/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1659 - accuracy: 0.9580\n",
            "Epoch 43/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1413 - accuracy: 0.9630\n",
            "Epoch 44/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0895 - accuracy: 0.9679\n",
            "Epoch 45/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0854 - accuracy: 0.9704\n",
            "Epoch 46/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1302 - accuracy: 0.9630\n",
            "Epoch 47/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0913 - accuracy: 0.9679\n",
            "Epoch 48/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1007 - accuracy: 0.9704\n",
            "Epoch 49/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1606 - accuracy: 0.9679\n",
            "Epoch 50/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.9679\n",
            "Epoch 51/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2206 - accuracy: 0.9580\n",
            "Epoch 52/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1827 - accuracy: 0.9506\n",
            "Epoch 53/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2035 - accuracy: 0.9654\n",
            "Epoch 54/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9432\n",
            "Epoch 55/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1575 - accuracy: 0.9531\n",
            "Epoch 56/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1756 - accuracy: 0.9580\n",
            "Epoch 57/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1137 - accuracy: 0.9654\n",
            "Epoch 58/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1619 - accuracy: 0.9679\n",
            "Epoch 59/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1228 - accuracy: 0.9630\n",
            "Epoch 60/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.9580\n",
            "Epoch 61/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1120 - accuracy: 0.9704\n",
            "Epoch 62/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2446 - accuracy: 0.9457\n",
            "Epoch 63/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1834 - accuracy: 0.9481\n",
            "Epoch 64/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0660 - accuracy: 0.9778\n",
            "Epoch 65/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1585 - accuracy: 0.9605\n",
            "Epoch 66/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1505 - accuracy: 0.9753\n",
            "Epoch 67/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1488 - accuracy: 0.9556\n",
            "Epoch 68/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2411 - accuracy: 0.9506\n",
            "Epoch 69/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1769 - accuracy: 0.9506\n",
            "Epoch 70/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0793 - accuracy: 0.9704\n",
            "Epoch 71/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1067 - accuracy: 0.9679\n",
            "Epoch 72/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1221 - accuracy: 0.9580\n",
            "Epoch 73/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1372 - accuracy: 0.9556\n",
            "Epoch 74/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1159 - accuracy: 0.9704\n",
            "Epoch 75/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1317 - accuracy: 0.9580\n",
            "Epoch 76/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9580\n",
            "Epoch 77/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1147 - accuracy: 0.9605\n",
            "Epoch 78/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0935 - accuracy: 0.9630\n",
            "Epoch 79/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1956 - accuracy: 0.9457\n",
            "Epoch 80/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0709 - accuracy: 0.9728\n",
            "Epoch 81/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1632 - accuracy: 0.9506\n",
            "Epoch 82/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1964 - accuracy: 0.9580\n",
            "Epoch 83/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.2377 - accuracy: 0.9506\n",
            "Epoch 84/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1475 - accuracy: 0.9654\n",
            "Epoch 85/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1605 - accuracy: 0.9580\n",
            "Epoch 86/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.2029 - accuracy: 0.9531\n",
            "Epoch 87/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1184 - accuracy: 0.9654\n",
            "Epoch 88/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3743 - accuracy: 0.9407\n",
            "Epoch 89/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0896 - accuracy: 0.9704\n",
            "Epoch 90/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1528 - accuracy: 0.9728\n",
            "Epoch 91/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.9728\n",
            "Epoch 92/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1053 - accuracy: 0.9704\n",
            "Epoch 93/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1639 - accuracy: 0.9556\n",
            "Epoch 94/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1752 - accuracy: 0.9679\n",
            "Epoch 95/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1671 - accuracy: 0.9481\n",
            "Epoch 96/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1577 - accuracy: 0.9580\n",
            "Epoch 97/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1597 - accuracy: 0.9580\n",
            "Epoch 98/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1898 - accuracy: 0.9531\n",
            "Epoch 99/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.2704 - accuracy: 0.9556\n",
            "Epoch 100/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1415 - accuracy: 0.9531\n",
            "Epoch 101/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0966 - accuracy: 0.9753\n",
            "Epoch 102/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1090 - accuracy: 0.9605\n",
            "Epoch 103/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1524 - accuracy: 0.9556\n",
            "Epoch 104/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1540 - accuracy: 0.9605\n",
            "Epoch 105/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1901 - accuracy: 0.9556\n",
            "Epoch 106/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1740 - accuracy: 0.9481\n",
            "Epoch 107/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1458 - accuracy: 0.9556\n",
            "Epoch 108/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9753\n",
            "Epoch 109/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 0.9580\n",
            "Epoch 110/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1209 - accuracy: 0.9580\n",
            "Epoch 111/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1457 - accuracy: 0.9605\n",
            "Epoch 112/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1338 - accuracy: 0.9580\n",
            "Epoch 113/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1227 - accuracy: 0.9654\n",
            "Epoch 114/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1410 - accuracy: 0.9654\n",
            "Epoch 115/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1575 - accuracy: 0.9605\n",
            "Epoch 116/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1662 - accuracy: 0.9630\n",
            "Epoch 117/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1479 - accuracy: 0.9605\n",
            "Epoch 118/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1534 - accuracy: 0.9605\n",
            "Epoch 119/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1851 - accuracy: 0.9531\n",
            "Epoch 120/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1554 - accuracy: 0.9531\n",
            "Epoch 121/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.3229 - accuracy: 0.9506\n",
            "Epoch 122/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1479 - accuracy: 0.9728\n",
            "Epoch 123/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0876 - accuracy: 0.9654\n",
            "Epoch 124/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1323 - accuracy: 0.9630\n",
            "Epoch 125/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2303 - accuracy: 0.9506\n",
            "Epoch 126/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.9679\n",
            "Epoch 127/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1514 - accuracy: 0.9556\n",
            "Epoch 128/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1890 - accuracy: 0.9531\n",
            "Epoch 129/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1762 - accuracy: 0.9432\n",
            "Epoch 130/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2080 - accuracy: 0.9531\n",
            "Epoch 131/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.9531\n",
            "Epoch 132/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1409 - accuracy: 0.9605\n",
            "Epoch 133/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1379 - accuracy: 0.9630\n",
            "Epoch 134/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1284 - accuracy: 0.9704\n",
            "Epoch 135/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0969 - accuracy: 0.9704\n",
            "Epoch 136/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1387 - accuracy: 0.9580\n",
            "Epoch 137/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0751 - accuracy: 0.9704\n",
            "Epoch 138/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0991 - accuracy: 0.9704\n",
            "Epoch 139/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2193 - accuracy: 0.9556\n",
            "Epoch 140/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1841 - accuracy: 0.9605\n",
            "Epoch 141/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1815 - accuracy: 0.9481\n",
            "Epoch 142/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1326 - accuracy: 0.9704\n",
            "Epoch 143/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0935 - accuracy: 0.9679\n",
            "Epoch 144/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1379 - accuracy: 0.9654\n",
            "Epoch 145/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1372 - accuracy: 0.9630\n",
            "Epoch 146/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1556 - accuracy: 0.9654\n",
            "Epoch 147/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1012 - accuracy: 0.9728\n",
            "Epoch 148/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1415 - accuracy: 0.9580\n",
            "Epoch 149/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1321 - accuracy: 0.9728\n",
            "Epoch 150/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9630\n",
            "Epoch 151/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0957 - accuracy: 0.9827\n",
            "Epoch 152/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0701 - accuracy: 0.9728\n",
            "Epoch 153/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9753\n",
            "Epoch 154/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1333 - accuracy: 0.9556\n",
            "Epoch 155/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9852\n",
            "Epoch 156/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0667 - accuracy: 0.9802\n",
            "Epoch 157/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1051 - accuracy: 0.9654\n",
            "Epoch 158/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1710 - accuracy: 0.9728\n",
            "Epoch 159/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1719 - accuracy: 0.9654\n",
            "Epoch 160/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1038 - accuracy: 0.9679\n",
            "Epoch 161/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1237 - accuracy: 0.9679\n",
            "Epoch 162/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1135 - accuracy: 0.9704\n",
            "Epoch 163/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1791 - accuracy: 0.9531\n",
            "Epoch 164/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1527 - accuracy: 0.9481\n",
            "Epoch 165/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1504 - accuracy: 0.9630\n",
            "Epoch 166/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1320 - accuracy: 0.9654\n",
            "Epoch 167/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1380 - accuracy: 0.9580\n",
            "Epoch 168/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1266 - accuracy: 0.9704\n",
            "Epoch 169/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1052 - accuracy: 0.9704\n",
            "Epoch 170/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0941 - accuracy: 0.9728\n",
            "Epoch 171/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1595 - accuracy: 0.9654\n",
            "Epoch 172/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1579 - accuracy: 0.9704\n",
            "Epoch 173/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0804 - accuracy: 0.9753\n",
            "Epoch 174/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1357 - accuracy: 0.9630\n",
            "Epoch 175/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1234 - accuracy: 0.9630\n",
            "Epoch 176/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1308 - accuracy: 0.9630\n",
            "Epoch 177/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1595 - accuracy: 0.9654\n",
            "Epoch 178/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9778\n",
            "Epoch 179/200\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1884 - accuracy: 0.9556\n",
            "Epoch 180/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1645 - accuracy: 0.9630\n",
            "Epoch 181/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9556\n",
            "Epoch 182/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1564 - accuracy: 0.9630\n",
            "Epoch 183/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1122 - accuracy: 0.9605\n",
            "Epoch 184/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1046 - accuracy: 0.9704\n",
            "Epoch 185/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1032 - accuracy: 0.9654\n",
            "Epoch 186/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 0.9802\n",
            "Epoch 187/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1465 - accuracy: 0.9654\n",
            "Epoch 188/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1455 - accuracy: 0.9630\n",
            "Epoch 189/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1150 - accuracy: 0.9679\n",
            "Epoch 190/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1177 - accuracy: 0.9704\n",
            "Epoch 191/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1064 - accuracy: 0.9728\n",
            "Epoch 192/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1335 - accuracy: 0.9605\n",
            "Epoch 193/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.2095 - accuracy: 0.9481\n",
            "Epoch 194/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1606 - accuracy: 0.9630\n",
            "Epoch 195/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1627 - accuracy: 0.9728\n",
            "Epoch 196/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0785 - accuracy: 0.9753\n",
            "Epoch 197/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1997 - accuracy: 0.9630\n",
            "Epoch 198/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1380 - accuracy: 0.9630\n",
            "Epoch 199/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1808 - accuracy: 0.9481\n",
            "Epoch 200/200\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9753\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chatbotModel.h5', hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jT0HPszgp-U"
      },
      "source": [
        "#ChatBot#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "049d4BCwgsEG",
        "outputId": "1d95406b-0a0b-4b6b-87ed-c654e70152fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GO!\n"
          ]
        }
      ],
      "source": [
        "words = pickle.load(open('words.pkl', 'rb'))\n",
        "classes = pickle.load(open('classes.pkl', 'rb'))\n",
        "model = load_model('./chatbotModel.h5')\n",
        "def clean_up_sentence(sentence):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
        "  return sentence_word\n",
        "\n",
        "def bag_of_words(sentence):\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "  bag = [0] * len(words)\n",
        "  for w in sentence_words:\n",
        "    for i, word in enumerate(words):\n",
        "      if word == w:\n",
        "        bag[i] = 1\n",
        "  return np.array(bag)\n",
        "\n",
        "def predict_class(sentence):\n",
        "  bow = bag_of_words(sentence)\n",
        "  res = model.predict(np.array([bow]))[0]\n",
        "  ERROR_THRESHOLD = 0.25\n",
        "  results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
        "\n",
        "  results.sort(key=lambda x: x[1], reverse=True)\n",
        "  return_list = []\n",
        "\n",
        "  for r in results:\n",
        "    return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n",
        "  return return_list\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "  tag = intents_list[0]['intent']\n",
        "  lsit_of_intents = intents_json['intents']\n",
        "  for i in list_of_intents:\n",
        "    if i['tag'] == tag:\n",
        "      result = random.choice(i['responses'])\n",
        "      break\n",
        "  return result\n",
        "\n",
        "print('GO!')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
